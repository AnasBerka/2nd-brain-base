[
  {"id":"agarwal_EfficientUseIoT_2016","accessed":{"date-parts":[["2025",3,24]]},"author":[{"family":"Agarwal","given":"Ankush"},{"family":"Gupta","given":"Shruti"},{"family":"Kumar","given":"Sandeep"},{"family":"Singh","given":"Dharmendra"}],"citation-key":"agarwal_EfficientUseIoT_2016","container-title":"2016 11th International Conference on Industrial and Information Systems (ICIIS)","container-title-short":"2016 11th Int. Conf. Ind. Inf. Syst. ICIIS","DOI":"10.1109/ICIINFS.2016.8263067","event-place":"Roorkee, India","event-title":"2016 11th International Conference on Industrial and Information Systems (ICIIS)","ISBN":"978-1-5090-3818-3","issued":{"date-parts":[["2016",12]]},"page":"905-909","publisher":"IEEE","publisher-place":"Roorkee, India","source":"DOI.org (Crossref)","title":"An efficient use of IoT for satellite data in land cover monitoring to estimate LST and ET","type":"paper-conference","URL":"http://ieeexplore.ieee.org/document/8263067/"},
  {"id":"ahmad_SurveyUsingDeep_2023","accessed":{"date-parts":[["2025",3,24]]},"author":[{"family":"Ahmad","given":"Aanis"},{"family":"Saraswat","given":"Dharmendra"},{"family":"El Gamal","given":"Aly"}],"citation-key":"ahmad_SurveyUsingDeep_2023","container-title":"Smart Agricultural Technology","container-title-short":"Smart Agricultural Technology","DOI":"10.1016/j.atech.2022.100083","ISSN":"27723755","issued":{"date-parts":[["2023",2]]},"language":"en","page":"100083","source":"DOI.org (Crossref)","title":"A survey on using deep learning techniques for plant disease diagnosis and recommendations for development of appropriate tools","type":"article-journal","URL":"https://linkinghub.elsevier.com/retrieve/pii/S277237552200048X","volume":"3"},
  {"id":"albanwan_ImageFusionRemote_2024","abstract":"Remote sensing image fusion is consistently used to turn raw images of different resolutions, sources, and modalities into accurate, complete, and spatiotemporally coherent images. It facilitates downstream applications such as pan sharpening, change detection, and classification. However,\n image fusion solutions are highly disparate to various remote sensing problems and are often narrowly defined in existing reviews as topical applications (e. g., pan sharpening). Theoretically, image fusion can be applied to any gridded data through pixel-level operations; thus, we expand\n its scope by comprehensively surveying relevant works. We develop a simple taxonomy for many-to-one and many-to-many image fusion, defining it as a mapping problem turning one or multiple images into another set based on desired coherence. Furthermore, we provide a meta-analysis to cover 10,420\n peer-reviewed papers from the 1980s to 2023 studying various types of image fusion and their applications. Finally, we discuss image fusion's benefits and emerging challenges to provide open research directions.","accessed":{"date-parts":[["2025",4,2]]},"author":[{"family":"Albanwan","given":"Hessah"},{"family":"Qin","given":"Rongjun"},{"family":"Tang","given":"Yang"}],"citation-key":"albanwan_ImageFusionRemote_2024","container-title":"Photogrammetric Engineering & Remote Sensing","container-title-short":"photogramm eng remote sensing","DOI":"10.14358/PERS.24-00110R1","ISSN":"0099-1112","issue":"12","issued":{"date-parts":[["2024",12,1]]},"language":"en","page":"755-775","source":"DOI.org (Crossref)","title":"Image Fusion in Remote Sensing: An Overview and Meta-Analysis","title-short":"Image Fusion in Remote Sensing","type":"article-journal","URL":"https://www.ingentaconnect.com/content/10.14358/PERS.24-00110R1","volume":"90"},
  {"id":"berka_CactiViTImagebasedSmartphone_2023","accessed":{"date-parts":[["2025",3,18]]},"author":[{"family":"Berka","given":"Anas"},{"family":"Hafiane","given":"Adel"},{"family":"Es-Saady","given":"Youssef"},{"family":"El Hajji","given":"Mohamed"},{"family":"Canals","given":"Raphaël"},{"family":"Bouharroud","given":"Rachid"}],"citation-key":"berka_CactiViTImagebasedSmartphone_2023","container-title":"Artificial Intelligence in Agriculture","container-title-short":"Artificial Intelligence in Agriculture","DOI":"10.1016/j.aiia.2023.07.002","ISSN":"25897217","issued":{"date-parts":[["2023",9]]},"language":"en","page":"12-21","source":"DOI.org (Crossref)","title":"CactiViT: Image-based smartphone application and transformer network for diagnosis of cactus cochineal","title-short":"CactiViT","type":"article-journal","URL":"https://linkinghub.elsevier.com/retrieve/pii/S2589721723000223","volume":"9"},
  {"id":"berka_EnhancingDeepLabV3Aerial_2024","accessed":{"date-parts":[["2025",3,18]]},"author":[{"family":"Berka","given":"Anas"},{"family":"Es-Saady","given":"Youssef"},{"family":"Hajji","given":"Mohamed El"},{"family":"Canals","given":"Raphael"},{"family":"Hafiane","given":"Adel"}],"citation-key":"berka_EnhancingDeepLabV3Aerial_2024","container-title":"2024 IEEE 12th International Symposium on Signal, Image, Video and Communications (ISIVC)","container-title-short":"2024 IEEE 12th Int. Symp. Signal Image Video Commun. ISIVC","DOI":"10.1109/ISIVC61350.2024.10577832","event-place":"Marrakech, Morocco","event-title":"2024 IEEE 12th International Symposium on Signal, Image, Video and Communications (ISIVC)","ISBN":"979-8-3503-8526-7","issued":{"date-parts":[["2024",5,21]]},"license":"https://doi.org/10.15223/policy-029","page":"1-6","publisher":"IEEE","publisher-place":"Marrakech, Morocco","source":"DOI.org (Crossref)","title":"Enhancing DeepLabV3+ for aerial image semantic segmentation using weighted upsampling","type":"paper-conference","URL":"https://ieeexplore.ieee.org/document/10577832/"},
  {"id":"berka_EnhancingDeepLabV3Fuse_2025","abstract":"Aerial and satellite imagery are inherently complementary remote sensing sources, offering high-resolution detail alongside expansive spatial coverage. However, the use of these sources for land cover segmentation introduces several challenges, prompting the development of a variety of segmentation methods. Among these approaches, the DeepLabV3+ architecture is considered as a promising approach in the field of single-source image segmentation. However, despite its reliable results for segmentation, there is still a need to increase its robustness and improve its performance. This is particularly crucial for multimodal image segmentation, where the fusion of diverse types of information is essential.\n An interesting approach involves enhancing this architectural framework through the integration of novel components and the modification of certain internal processes.\n In this paper, we enhance the DeepLabV3+ architecture by introducing a new transposed conventional layers block for upsampling a second entry to fuse it with high level features. This block is designed to amplify and integrate information from satellite images, thereby enriching the segmentation process through fusion with aerial images.\n For experiments, we used the LandCover.ai (Land Cover from Aerial Imagery) dataset for aerial images, alongside the corresponding dataset sourced from Sentinel 2 data.\n Through the fusion of both sources, the mean Intersection over Union (mIoU) achieved a total mIoU of 84.91% without data augmentation.","accessed":{"date-parts":[["2025",4,1]]},"author":[{"family":"Berka","given":"Anas"},{"family":"El Hajji","given":"Mohamed"},{"family":"Canals","given":"Raphael"},{"family":"Es-saady","given":"Youssef"},{"family":"Hafiane","given":"Adel"}],"citation-key":"berka_EnhancingDeepLabV3Fuse_2025","DOI":"10.48550/ARXIV.2503.22909","issued":{"date-parts":[["2025"]]},"license":"arXiv.org perpetual, non-exclusive license","publisher":"arXiv","source":"DOI.org (Datacite)","title":"Enhancing DeepLabV3+ to Fuse Aerial and Satellite Images for Semantic Segmentation","type":"article","URL":"https://arxiv.org/abs/2503.22909","version":"1"},
  {"id":"berka_SmartFarmingSysteme_2024","author":[{"family":"Berka","given":"Anas"}],"citation-key":"berka_SmartFarmingSysteme_2024","contributor":[{"family":"Hafiane","given":"Adel; Es-Saady","suffix":"Youssef"}],"issued":{"date-parts":[["2024"]]},"source":"http://www.theses.fr/2024ISAB0012","title":"Smart farming : Système d’aide à la décision basé sur la fusion de données multi-sources","type":"thesis","URL":"http://www.theses.fr/2024ISAB0012/document"},
  {"id":"chen_DCKTNovelDualCentric_2022","abstract":"Knowledge tracing (KT), aiming to model learners’ mastery of a concept based on their historical learning records, has received extensive attention due to its great potential in realizing personalized learning in intelligent tutoring systems. However, most existing KT methods focus on a single aspect of knowledge or learner, not paying careful attention to the coupling influence of knowledge and learner characteristics. To fill this gap, in this paper, we explore a new paradigm for the KT task by exploiting the coupling influence of knowledge and learner. A novel model called Dual-Centric Knowledge Tracing (DCKT) is proposed to model knowledge states through two joint tasks of knowledge modeling and learner modeling. In particular, we first generate concept embeddings in abundant knowledge structure information via a pretext task (knowledge-centric): unsupervised graph representation learning. Then, we deeply measure learners’ prior knowledge the knowledge-enhanced representations and three predefined educational priors for discriminative feature enhancement. Furthermore, we design a forgetting-fusion transformer (learner-centric) to simulate the declining trend of learners’ knowledge proficiency over time, representing the common forgetting phenomenon. Extensive experiments were conducted on four public datasets, and the results demonstrate that DCKT could achieve better knowledge tracing results over all datasets via a dual-centric modeling process. Additionally, DCKT can learn meaningful question embeddings automatically without manual annotations. Our work indicates a potential future research direction for personalized learner modeling, which is of both accuracy and high interpretability.","accessed":{"date-parts":[["2025",3,11]]},"author":[{"family":"Chen","given":"Yixuan"},{"family":"Wang","given":"Shuang"},{"family":"Jiang","given":"Fan"},{"family":"Tu","given":"Yaxin"},{"family":"Huang","given":"Qionghao"}],"citation-key":"chen_DCKTNovelDualCentric_2022","container-title":"Sustainability","container-title-short":"Sustainability","DOI":"10.3390/su142316307","ISSN":"2071-1050","issue":"23","issued":{"date-parts":[["2022",12,6]]},"language":"en","license":"https://creativecommons.org/licenses/by/4.0/","page":"16307","source":"DOI.org (Crossref)","title":"DCKT: A Novel Dual-Centric Learning Model for Knowledge Tracing","title-short":"DCKT","type":"article-journal","URL":"https://www.mdpi.com/2071-1050/14/23/16307","volume":"14"},
  {"id":"chen_MultisourceRemotelySensed_2017","accessed":{"date-parts":[["2025",3,24]]},"author":[{"family":"Chen","given":"Bin"},{"family":"Huang","given":"Bo"},{"family":"Xu","given":"Bing"}],"citation-key":"chen_MultisourceRemotelySensed_2017","container-title":"ISPRS Journal of Photogrammetry and Remote Sensing","container-title-short":"ISPRS Journal of Photogrammetry and Remote Sensing","DOI":"10.1016/j.isprsjprs.2016.12.008","ISSN":"09242716","issued":{"date-parts":[["2017",2]]},"language":"en","page":"27-39","source":"DOI.org (Crossref)","title":"Multi-source remotely sensed data fusion for improving land cover classification","type":"article-journal","URL":"https://linkinghub.elsevier.com/retrieve/pii/S0924271616306463","volume":"124"},
  {"id":"chen_RobustYOLOv5Model_2025","abstract":"This study presents an improved detection model based on the YOLOv5 (You Only Look Once version 5) framework to enhance the accuracy of Jishan jujube detection in complex natural environments, particularly with varying degrees of occlusion and dense foliage. To improve detection performance, we integrate an SE (squeeze-and-excitation) attention module into the backbone network to enhance the model’s ability to focus on target objects while suppressing background noise. Additionally, the original neck network is replaced with a BIFPN (bi-directional feature pyramid network) structure, enabling efficient multiscale feature fusion and improving the extraction of critical features, especially for small and occluded fruits. The experimental results demonstrate that the improved YOLOv5 model achieves a mean average precision (mAP) of 96.5%, outperforming the YOLOv3, YOLOv4, YOLOv5, and SSD (Single-Shot Multibox Detector) models by 7.4%, 9.9%, 2.5%, and 0.8%, respectively. Furthermore, the proposed model improves precision (95.8%) and F1 score (92.4%), reducing false positives and achieving a better balance between precision and recall. These results highlight the model’s effectiveness in addressing missed detections of small and occluded fruits while maintaining higher confidence in predictions.","accessed":{"date-parts":[["2025",4,1]]},"author":[{"family":"Chen","given":"Hao"},{"family":"Su","given":"Lijun"},{"family":"Tian","given":"Yiren"},{"family":"Chai","given":"Yixin"},{"family":"Hu","given":"Gang"},{"family":"Mu","given":"Weiyi"}],"citation-key":"chen_RobustYOLOv5Model_2025","container-title":"Agriculture","container-title-short":"Agriculture","DOI":"10.3390/agriculture15060665","ISSN":"2077-0472","issue":"6","issued":{"date-parts":[["2025",3,20]]},"language":"en","license":"https://creativecommons.org/licenses/by/4.0/","page":"665","source":"DOI.org (Crossref)","title":"A Robust YOLOv5 Model with SE Attention and BIFPN for Jishan Jujube Detection in Complex Agricultural Environments","type":"article-journal","URL":"https://www.mdpi.com/2077-0472/15/6/665","volume":"15"},
  {"id":"hong_SpectralFormerRethinkingHyperspectral_2022","abstract":"Hyperspectral (HS) images are characterized by approximately contiguous spectral information, enabling the fine identification of materials by capturing subtle spectral discrepancies. Owing to their excellent locally contextual modeling ability, convolutional neural networks (CNNs) have been proven to be a powerful feature extractor in HS image classification. However, CNNs fail to mine and represent the sequence attributes of spectral signatures well due to the limitations of their inherent network backbone. To solve this issue, we rethink HS image classification from a sequential perspective with transformers, and propose a novel backbone network called \\ul{SpectralFormer}. Beyond band-wise representations in classic transformers, SpectralFormer is capable of learning spectrally local sequence information from neighboring bands of HS images, yielding group-wise spectral embeddings. More significantly, to reduce the possibility of losing valuable information in the layer-wise propagation process, we devise a cross-layer skip connection to convey memory-like components from shallow to deep layers by adaptively learning to fuse \"soft\" residuals across layers. It is worth noting that the proposed SpectralFormer is a highly flexible backbone network, which can be applicable to both pixel- and patch-wise inputs. We evaluate the classification performance of the proposed SpectralFormer on three HS datasets by conducting extensive experiments, showing the superiority over classic transformers and achieving a significant improvement in comparison with state-of-the-art backbone networks. The codes of this work will be available at https://github.com/danfenghong/IEEE_TGRS_SpectralFormer for the sake of reproducibility.","accessed":{"date-parts":[["2025",3,24]]},"author":[{"family":"Hong","given":"Danfeng"},{"family":"Han","given":"Zhu"},{"family":"Yao","given":"Jing"},{"family":"Gao","given":"Lianru"},{"family":"Zhang","given":"Bing"},{"family":"Plaza","given":"Antonio"},{"family":"Chanussot","given":"Jocelyn"}],"citation-key":"hong_SpectralFormerRethinkingHyperspectral_2022","container-title":"IEEE Transactions on Geoscience and Remote Sensing","container-title-short":"IEEE Trans. Geosci. Remote Sensing","DOI":"10.1109/TGRS.2021.3130716","ISSN":"0196-2892, 1558-0644","issued":{"date-parts":[["2022"]]},"page":"1-15","source":"arXiv.org","title":"SpectralFormer: Rethinking Hyperspectral Image Classification with Transformers","title-short":"SpectralFormer","type":"article-journal","URL":"http://arxiv.org/abs/2107.02988","volume":"60"},
  {"id":"kamilaris_DeepLearningAgriculture_2018","accessed":{"date-parts":[["2025",3,24]]},"author":[{"family":"Kamilaris","given":"Andreas"},{"family":"Prenafeta-Boldú","given":"Francesc X."}],"citation-key":"kamilaris_DeepLearningAgriculture_2018","container-title":"Computers and Electronics in Agriculture","container-title-short":"Computers and Electronics in Agriculture","DOI":"10.1016/j.compag.2018.02.016","ISSN":"01681699","issued":{"date-parts":[["2018",4]]},"language":"en","page":"70-90","source":"DOI.org (Crossref)","title":"Deep learning in agriculture: A survey","title-short":"Deep learning in agriculture","type":"article-journal","URL":"https://linkinghub.elsevier.com/retrieve/pii/S0168169917308803","volume":"147"},
  {"id":"khalid_DeepLearningPlant_2023","abstract":"Agriculture, an essential bedrock of human survival, continually grapples with the menace of plant diseases, culminating in substantial yield reductions. While conventional detection techniques remain widespread, they often entail laborious efforts and are susceptible to inaccuracies, underscoring the pressing need for more efficient, scalable, and immediate solutions. Our research explores the transformative capabilities of Deep Learning (DL) models, primarily focusing on Convolutional Neural Networks (CNNs) and MobileNet architectures in the early and precise identification of plant ailments. We augmented our exploration by incorporating eXplainable Artificial Intelligence (XAI) through GradCAM, which elucidated the decision-making process of these models, providing a visual interpretation of disease indicators in plant images. Through rigorous testing, our CNN model yielded an accuracy of 89%, a precision and recall of 96%, and an F1-score of 96%. Conversely, the MobileNet design showcased an accuracy of 96% but recorded slightly lesser precision, recall, and F1-scores of 90%, 89%, and 89%, respectively. Such results amplify the transformative role of DL in redefining plant disease detection methodologies, presenting a formidable counterpart to conventional techniques and ushering in an era of heightened agricultural security.","accessed":{"date-parts":[["2025",3,24]]},"author":[{"family":"Khalid","given":"Munaf Mudheher"},{"family":"Karan","given":"Oguz"}],"citation-key":"khalid_DeepLearningPlant_2023","container-title":"International Journal of Mathematics, Statistics, and Computer Science","container-title-short":"ijmscs","DOI":"10.59543/ijmscs.v2i.8343","ISSN":"2704-1069, 2704-1077","issued":{"date-parts":[["2023",11,18]]},"license":"https://creativecommons.org/licenses/by/4.0","page":"75-84","source":"DOI.org (Crossref)","title":"Deep Learning for Plant Disease Detection: Deep Learning for Plant","title-short":"Deep Learning for Plant Disease Detection","type":"article-journal","URL":"https://ijmscs.org/index.php/ijmscs/article/view/8343","volume":"2"},
  {"id":"li_DRPLDeepRegression_2020","abstract":"In this paper, a novel deep network is proposed for multi-focus image fusion, named Deep Regression Pair Learning (DRPL). In contrast to existing deep fusion methods which divide the input image into small patches and apply a classifier to judge whether the patch is in focus or not, DRPL directly converts the whole image into a binary mask without any patch operation, subsequently tackling the difficulty of the blur level estimation around the focused/defocused boundary. Simultaneously, a pair learning strategy, which takes a pair of complementary source images as inputs and generates two corresponding binary masks, is introduced into the model, greatly imposing the complementary constraint on each pair and making a large contribution to the performance improvement. Furthermore, as the edge or gradient does exist in the focus part while there is no similar property for the defocus part, we also embed a gradient loss to ensure the generated image to be all-in-focus. Then the structural similarity index (SSIM) is utilized to make a trade-off between the reference and fused images. Experimental results conducted on the synthetic and real-world datasets substantiate the effectiveness and superiority of DRPL compared with other state-of-the-art approaches. The source code can be found in https://github.com/sasky1/DPRL.","accessed":{"date-parts":[["2025",5,19]]},"author":[{"family":"Li","given":"Jinxing"},{"family":"Guo","given":"Xiaobao"},{"family":"Lu","given":"Guangming"},{"family":"Zhang","given":"Bob"},{"family":"Xu","given":"Yong"},{"family":"Wu","given":"Feng"},{"family":"Zhang","given":"David"}],"citation-key":"li_DRPLDeepRegression_2020","container-title":"IEEE Transactions on Image Processing","container-title-short":"IEEE Trans. Image Process.","DOI":"10.1109/TIP.2020.2976190","ISSN":"1941-0042","issued":{"date-parts":[["2020"]]},"page":"4816-4831","source":"IEEE Xplore","title":"DRPL: Deep Regression Pair Learning for Multi-Focus Image Fusion","title-short":"DRPL","type":"article-journal","URL":"https://ieeexplore.ieee.org/document/9020016","volume":"29"},
  {"id":"munoz_LightweightDeepLabv3Semantic_2025","abstract":"Advancements in artificial intelligence, particularly in computer vision, have driven the research and development of visual food analysis systems focused primarily on enhancing people’s well-being. Food analysis can be performed at various levels of granularity, with food segmentation  being a major component for numerous real-world applications. Deep learning-based methods  have demonstrated promising results in food segmentation; however, many of these approaches  demand high computational resources, making them impractical for low-performance devices. In  this research, a novel lightweight deep learning-based method for semantic food segmentation is  proposed. To achieve this, the state-of-the-art DeepLabv3+ model was adapted by optimizing the backbone, neck, and output of the encoder. To validate the method, four publicly available food datasets were selected. Additionally, a new food segmentation dataset consisting of self-acquired food images was introduced and included in the validation. The results demonstrate that high performance can be achieved at a significantly lower cost. The proposed method yields results that are either better or comparable to those of the state-of-the-art techniques while requiring significantly less computational cost. In conclusion, although there is still room for improvement, the results are promising for performing food image segmentation on low-performance stand-alone devices.","author":[{"family":"Muñoz","given":"Bastián"},{"family":"Martínez-Arroyo","given":"Angela"},{"family":"Acevedo","given":"Constanza"},{"family":"Aguilar","given":"Eduardo"}],"citation-key":"munoz_LightweightDeepLabv3Semantic_2025","issued":{"date-parts":[["2025"]]},"language":"en","source":"Zotero","title":"Lightweight DeepLabv3+ for Semantic Food Segmentation","type":"article-journal"},
  {"id":"ravpreet_DeepLabv3CBAMBased_2025","abstract":"Multi-focus image fusion (MFIF) technique aims to produce a full focused image that preserves the details from source images and enhance image fusion performance. A significant problem in MFIF is correctly segmenting the focused region, particularly in complicated images with challenging boundaries. This limitation causes a loss of critical image information, resulting in inferior fusion quality. The existing semantic segmentation methods: FCN, PSPNet and UNet frequently have difficulty with this task. To overcome these difficulties, this work presents a framework employing DeepLabv3+ architecture with Convolutional Block Attention Module (CBAM), named DLCMF. The CBAM improves the framework’s ability using channel and spatial attention to focus on key characteristics in an image, effectively capturing “what” and “where” to look in an image. This helps to preserve edge details and improves feature extraction accuracy, decreasing the loss of critical information during the process of fusion. To optimize training, both cross-entropy loss and dice loss functions are employed. The complementary source images are fed to DLCMF network which generates binary pair segmentation maps that are subsequently refined using morphological operations to yield final segmentation maps. Finally, the dot product is computed between the inputs and their corresponding binary map followed by a pixel wise summation. Experimental findings indicate that DLCMF generates fused images of higher quality compared to those generated by other nine existing state-of-the-art methods","author":[{"family":"Ravpreet","given":"Kaur"},{"family":"Sarbjee","given":"Singht"}],"citation-key":"ravpreet_DeepLabv3CBAMBased_2025","genre":"Regular Paper","issued":{"date-parts":[["2025"]]},"language":"Eng","number":"DSP-D-25-00624","publisher":"Digital Signal Processing","title":"A DeepLabv3+ and CBAM based framework for Multi-focus image fusion","type":"article"},
  {"id":"saha_IOTbasedDroneImprovement_2018","accessed":{"date-parts":[["2025",3,24]]},"author":[{"family":"Saha","given":"Arnab Kumar"},{"family":"Saha","given":"Jayeeta"},{"family":"Ray","given":"Radhika"},{"family":"Sircar","given":"Sachet"},{"family":"Dutta","given":"Subhojit"},{"family":"Chattopadhyay","given":"Soummyo Priyo"},{"family":"Saha","given":"Himadri Nath"}],"citation-key":"saha_IOTbasedDroneImprovement_2018","container-title":"2018 IEEE 8th Annual Computing and Communication Workshop and Conference (CCWC)","container-title-short":"2018 IEEE 8th Annu. Comput. Commun. Workshop Conf. CCWC","DOI":"10.1109/CCWC.2018.8301662","event-place":"Las Vegas, NV","event-title":"2018 IEEE 8th Annual Computing and Communication Workshop and Conference (CCWC)","ISBN":"978-1-5386-4649-6","issued":{"date-parts":[["2018",1]]},"page":"612-615","publisher":"IEEE","publisher-place":"Las Vegas, NV","source":"DOI.org (Crossref)","title":"IOT-based drone for improvement of crop quality in agricultural field","type":"paper-conference","URL":"http://ieeexplore.ieee.org/document/8301662/"},
  {"id":"schieber_DeepSensorFusion_2022","accessed":{"date-parts":[["2025",3,20]]},"author":[{"family":"Schieber","given":"Hannah"},{"family":"Duerr","given":"Fabian"},{"family":"Schoen","given":"Torsten"},{"family":"Beyerer","given":"Jurgen"}],"citation-key":"schieber_DeepSensorFusion_2022","container-title":"2022 IEEE Intelligent Vehicles Symposium (IV)","container-title-short":"2022 IEEE Intell. Veh. Symp. IV","DOI":"10.1109/IV51971.2022.9827113","event-place":"Aachen, Germany","event-title":"2022 IEEE Intelligent Vehicles Symposium (IV)","ISBN":"978-1-6654-8821-1","issued":{"date-parts":[["2022",6,5]]},"license":"https://doi.org/10.15223/policy-029","page":"375-381","publisher":"IEEE","publisher-place":"Aachen, Germany","source":"DOI.org (Crossref)","title":"Deep Sensor Fusion with Pyramid Fusion Networks for 3D Semantic Segmentation","type":"paper-conference","URL":"https://ieeexplore.ieee.org/document/9827113/"},
  {"id":"zhao_MultisourceRemoteSensing_2019","accessed":{"date-parts":[["2025",3,24]]},"author":[{"family":"Zhao","given":"Xudong"},{"family":"Tao","given":"Ran"},{"family":"Li","given":"Wei"}],"citation-key":"zhao_MultisourceRemoteSensing_2019","container-title":"ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","container-title-short":"ICASSP 2019 - 2019 IEEE Int. Conf. Acoust. Speech Signal Process. ICASSP","DOI":"10.1109/ICASSP.2019.8683032","event-place":"Brighton, United Kingdom","event-title":"ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","ISBN":"978-1-4799-8131-1","issued":{"date-parts":[["2019",5]]},"license":"https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html","page":"2187-2191","publisher":"IEEE","publisher-place":"Brighton, United Kingdom","source":"DOI.org (Crossref)","title":"Multisource Remote Sensing Data Classification Using Deep Hierarchical Random Walk Networks","type":"paper-conference","URL":"https://ieeexplore.ieee.org/document/8683032/"}
]
